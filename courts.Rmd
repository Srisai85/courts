---
Title: "Predicting US SupremeCourt Decision Making- Motivation and Model Building"
Author: "Srisai Sivakumar"
Date: "Friday, July 17, 2015"
Output:
  html_document:
    pandoc_args: [
      "+RTS", "-K64m",
      "-RTS"
    ]
---
# Predicting US SupremeCourt Decision Making

## By: Srisai Sivakumar

### Part 1: Introduction, description, assumptions, models and tuning

This is the first of the 3 part series on using machine learning algorithms to predict the juedegemt of the cases appearing before the Supreme Court of the USA. If the models give rockstar grade predictions, perhaps, one day, the courts can be run by R codes and excel sheets?

### Motivation

Political scientists and legal academics have long scrutinized the legal entities like Court with the intent to understand what motivates the justices and get insights into predicting the coutcomes of cases.  Lawyers, too, possess expertise that should enable them to forecast legal events with some accuracy. After all, the everyday practice of law requires lawyers to predict court decisions in order to advise clients or determine suitable strategies. Prediction of success in the case is of paramount importance for several reasons. In the course of litigation, lawyers constantly make strategic decisions and/or advise their clients on the basis of these predictions. Attorneys make decisions about future courses of action, such as whether to take on a new client, the value of a case, whether to advise the client to enter into settlement negotiations, and whether to accept a settlement offer or proceed to trial. Thus, these
judgments are influential in shaping the cases and the mechanisms selected to resolve them. Clients' choices and outcomes therefore depend on the abilities of their counsel to make reasonably accurate forecasts
concerning case outcomes. 

My uncle, who is a lawyer in India, says that in civil cases, after depositions of key witnesses or at the close of discovery, the parties reassess the likelihood of success at trial in light of the impact of these events. Ability to foresee or predict the outcomes/verdict is an invaluable insight not just to the client, but also for the possible lawyer, whose professional and financial success, satisfaction of his client is strongly correlated to his ability to predict the outcome of the case. 

This study examines the cases using statistical models that relies on the general case characteristics.

### Assumption

Our principal goal in constructing statistical models capable of predicting the outcome of Supreme Court cases "prospectively"", using only information available prior to the judgement or oral argument. 

A cornerstone of creating such statistical models is an assumption about
the temporal stability in the Justices' behavior. In other words, its assumed that observable patterns in the Justices' past behavior would hold true for their future behavior.  

### Set-up

This study attempts to predict the outcome of cases heard by the US Supreme Court. The Statistical forecasting model is based on information derived from past Supreme Court decisions. The model discerns patterns in the justices' decisions based on observable case characteristics. Bases on these patterns, the model allows the construction of classification trees and other such models to predict outcomes based on these characteristics
of the cases. This correctness or goodness of the model shall be evaluated by allowing it to predict the outcomes of 'settled' cases. Shoult this accuracy be good enough, the model may be used to predict the verdicts of future or unresolved cases.

### Brief description of US judicial setup and summary of dataset

The legal system of the United States operates at the state level and at the federal level. Federal courts hear cases beyond the scope of state law. Federal courts are divided into:

- District Courts: makes initial decision

- Circuit Courts: hears appeals from the district courts

- Supreme Court: highest level - makes final decision 

The Supreme Court of the United States Consists of nine judges ("justices"), appointed by the President. Justices are distinguished judges, professors of law, state and federal attorneys. The final verdict is the collective judgement is taken by a 'bench' of judges. Predictind the collective outcome needs predicting the juegements of all the 3 judges in the bench. It can be thought of predicting the decisions of one judge and using the same process to the other two. In essence, the framework to predict the judgements of one judge can be extended to include multiple.

This study will focus on the judgements of one such judge, Justice Stevens. We examine his judgements in cases from 1994 through 2001, totalling to 567 cases.

### The data and assumptions

The data set has 567 cases. The variables in the data set are:

- Docket: docket number

- Term: year of hearing (e.g., 1999, 2001, etc)

- Circuit: circuit of origin (1st - 11th, DC, FED)

- Issue: issue area of the case.

- Petitioner: type of petitioner  (e.g., the United States, an employer, etc.).

- Respondent: type of respondent (e.g., Americal Indian, Business, etc.).

- LowerCourt:  ideological direction (liberal or conservative) of the lower
court ruling.

- Unconst:  whether the petitioner argued that a law or practice is unconstitutional.

- Reverse:  affirm/reverse judgement of lower court.


```{r read_files}
setwd("~/working_directory/R/AE/Trees")
# read csv file
stevens1 = read.csv("stevens.csv")
# examining the structure of the data frame
str(stevens1)
```

We have assumed termporal stability of the judge. This means that the year of hearing has no bearing on the outcome of the judge. It is also fair to assume that the judge will reverse or affirm the ruling of a lower court if he deems necessary, irrespective of the docket number of the case. So we remove the variables 'Docket' and 'Term' from the data frame.

The dependent variable, 'Reverse' is categorized '0' and '1'. It may be noted from the str output that this variable is of type integer. Its useful to have them as a caregorical variable. So we convery them into a factor, with 'yes' replacing 1, and 'no', 0.

```{r clean_files}
# converting dependent variable (Reverse) to factor.
stevens1$Reverse <- factor(ifelse(stevens1$Reverse == 1, "yes","no"))
# transforming the unconst variable as a factor too.
stevens1$Unconst = as.factor(stevens1$Unconst)
# removing Docket and Term variables from the data frame
stevens = stevens1[,-c(1,2)]
# examining the structure of the new data frame
str(stevens)
```

### Training and Test data sets

We now have the data in hand and we can proceed with model building. The primary tools of choice would be the options from the Caret , that provides a unified framework for numerous machine learning algorithms.

Before we split the data into training and test set, its important to give this step a thought. We are trying to build a model that would predict the outcomes of cases in coing years. Currently, the most prevalent method to get the training and test data is to split the data ramdomly into training set and test set. But one must think how predicting the outcome of a case from 1996 would give confidence in the model's ability to predict the outcome of a case in 2002. A possible way around is to see if we can split the data by years or term of the cases' hearing. We can make a training set out of the cases from 1994-1999 and test set from the cases 2000-01. But with this approach, we have to ensure that the test set have any 'new' kind or class of case that the training test doesnt have. This is a hasslenot worth endring, especially with the assumption of temporal stability of the judge. So its fair to assume that the year or term of hearing has no bearing or influence on the precitions of the outcome. So we stick to the widely used random subsetting to get the training and test sets.

As we proceed with the spits, we set seed to enable reproducable results. We split the data into training and test sets, with 70% and 30% of the data respectively.

```{r train_test, warning=FALSE, message=FALSE}
library(caret)
# set seed - 3000
set.seed(3000)
inTraining <- createDataPartition(stevens$Reverse, p = .7, list = FALSE)
Train <- stevens[ inTraining,]
Test  <- stevens[-inTraining,]
str(Train)
```

Now that the training and test sets are ready, we can go ahead with model building.

### Models

Logistic regression is likely to provide a good starting point for this binary classification problem. The linear nature of the algorithm might limit the accuracy of the prediction, but the more important concern is interpretability of the final model, especially interpreting the coefficents by non-technical or upper management people. It also needs more thought in interpreting which factors are important and critically, predicting the outcome without a 'tree' or a graph structure to help.

#### Baseline

Before we jump into model building, its important we understand the purpose of building the model. We try to biuld the model to predict the outcomes of Justice Stevens. It can be taken for granted that no model is be 100% accurate. But what accuracy can be termed 'good'? Its important to resolve this before we start making predictions.

A good starting point is 'naive' prediction. By naive, I mean the lack of sophestication in the process of making a prediction.

This can be done in 2 ways.

First, randomly select if the verdict of the lower court is overruled by the Supreme Court. Compare this with the training data. Since there is randomness in the process, its worth averaging it over, say 100 iterations.

```{r random_bl}
b = rep(NA,100)
for (i in 1:100){
        b[i] = mean((sample(c("yes","no"),nrow(Train),replace=T) == Train$Reverse))
}
rand_bl = mean(b)
print(paste("the random baseline is:",as.character(round(rand_bl,3))))
```

Unsurprisingly, this gives a accuracy of close to 0.5. This is what one would expect in a random process.

Another naive way of doing this is to assume all the verdicts of the cases are same and is to reverse the ruling of the lower court.

```{r baseline}

baseline = mean((Train$Reverse == 'yes'))

print(paste("the baseline is:",as.character(round(baseline,3))))

```
This is a better baseline than random choice. We would use this to evaluate the goodness of the predicting algorithm.

#### Model Building

We begin with one of the more interpretable models, CART. This provides a easily interpretable solution at the possible expense of accuracy. We biuld a model without tuning. And then we proceed to tune the model in search of better results.

#### Classification Trees- CART

The caret package, by default, uses simple bootstrap resampling method to automatically choose the tuning parameters associated with the best value. Different algorithms such as repeated K-fold cross-validation, leave-one-out, etc. can be used as well and will be examined in the subsequent sections.

```{r tree}

# No tune model
set.seed(3000)
# train the model using training set Train and method as 'rpart' for classification trees
tree = train(Reverse ~ ., data = Train, method="rpart")
tree
# predict the outcomes of the test set, Test.
predicttree = predict(tree, newdata = Test)
confusionMatrix(Test$Reverse, predicttree)

accuracy_tree = confusionMatrix(Test$Reverse, predicttree)$overall[1]
names(accuracy_tree) <- NULL

print(paste0("The untuned Tree model gives an accuracy of ", as.character(round(accuracy_tree,4)),", an improvement over the baseline of ",as.character(round((accuracy_tree/baseline - 1 ) * 100,2)),'%'))

```

The last chunk of the post processing script would be used for all the models and would be convenient to have it as a function.

```{r post}
postProcess <- function(model,Test=Test) {
        pred <- predict(model, newdata = Test)
        print(confusionMatrix(Test$Reverse,pred))
        accuracy = confusionMatrix(Test$Reverse, pred)$overall[1]
        names(accuracy) <- NULL
        print(paste0("Model ",deparse(substitute(model)) , " gives an accuracy of ", as.character(round(accuracy,4)),", an improvement over the baseline of ",as.character(round((accuracy/baseline - 1 ) * 100,2)),'%'))
}
```

Back to the tree model, we have a working tree model that we would like to view and understand how the prediction works. We can get the tuning parameter values from the display above. 

We now tune the Tree model in search of further improvement.

We now use 10-fold cross validation as the resampling method and establish 15 tuning points to the model, using the tuneLength option. 

```{r tree1, echo=FALSE,message=FALSE,warning=FALSE}
fitControl <- trainControl(method ="cv",number = 10)
set.seed(3000)
tree1 <- train(Reverse ~ ., data = Train, method = "rpart", trControl = fitControl,tuneLength = 15)
tree1
postProcess(tree1,Test)
```

It looks like the tree we have reached the maximum performance of the tree models. To check, lets focus on the cp values that gives best performance in both the previous models. It was 0.04444444 and 0.1571429

It may be worth establishing a cp tuning grid in the interval of 0.00 and 0.4, in steps of 0.02. This can be given the the model using the tuneGrid function.

```{r tree2, echo=FALSE,message=FALSE,warning=FALSE}
set.seed(3000)
tree2 <- train(Reverse ~ ., data = Train, method = "rpart", trControl = fitControl,tuneGrid = expand.grid( .cp = seq(0,0.4,by=0.02)))
tree2
postProcess(tree2,Test)
```

It is fair to conclude that we have reached the max performance of the CART model for this data set.

#### Random Forest

Lets look at an extension of the tree based models, the Random Forest. Random forest builds numerous trees, with a set (mtree) number of variables randomly chosen to model each split.

Lets first begin with the basic random forest model, without any tuning.

```{r rf, echo=FALSE,warning=FALSE,message=FALSE}
set.seed(3000)
rf <- train(Reverse ~., data = Train, method = "rf",do.trace=F,allowParallel=F)
rf
postProcess(rf,Test)
```

This is lower than the accuracy of the Cart models. To understand if the default resampling technique of bootstrapping has any effect on the accuracy, we specify the resampling method to be 10-fold CV.

```{r rf1, echo=FALSE,warning=FALSE,message=FALSE}
set.seed(3000)
rf1 <- train(Reverse ~., data = Train, method = "rf",do.trace=F,
             trControl=fitControl,allowParallel=F)
rf1
postProcess(rf1,Test)
```

10-fold CV does improve the performance of the model, albeit marginally. Lets try to tune the model and see if that offers any improvement in prediction accuracy.

```{r rf2, echo=FALSE,warning=FALSE,message=FALSE}
set.seed(3000)
rf2 <- train(Reverse ~., data = Train, method = "rf",do.trace=F,
             ntree= 250,tuneGrid = expand.grid(.mtry = seq(2,50,by=2)),
             trControl=fitControl, preProcess = c("center","scale") ,allowParallel=F)
rf2
postProcess(rf2,Test)
```

It seems that we have hit the max performance of random forest. Surprisingly random forest performs worse than CART. Though RFs were introduced to deal with issues of overfitting in normal decision trees but this doesn't happen on every data set. It looks like RF overfits the data, which is evident from the fact that rf has higher training set accuracy, but lower test set accuracy. 

Lets examine a similar forest technique, called the cforest.

```{r cf, echo=FALSE,warning=FALSE,message=FALSE}
set.seed(3000)
cf = train(Reverse ~ ., data=Train, method = "cforest")
cf
postProcess(cf,Test)

```

This gives very good prediction already. But lets explore if we can make it even better by resapling.

```{r cf1, echo=FALSE,warning=FALSE,message=FALSE}
fitControl2 = trainControl(method = "cv", number = 10, classProbs = TRUE,
                           summaryFunction = twoClassSummary)
set.seed(3000)
cf1 = train(Reverse ~ ., data=Train, method = "cforest",
            trControl=fitControl2)
cf1
postProcess(cf1,Test)

```
This is the most accurate prediction yet. 

Lets proceed to the next tree based technique.

#### Boosting

We start investigating the other tree based model, Boosting. As with the other models, we begin with a no-tune model. 

```{r gbm, echo=FALSE,warning=FALSE,message=FALSE}
set.seed(3000)
gbm<-train(Reverse ~ .,data=Train,method="gbm",verbose = FALSE)
gbm
postProcess(gbm,Test)
```

This gives an accuracy of close to 0.7. This is already superior to the prediction of tuned RF. Lets proceed with tuning the boosting model. Lets begin with using 10-fold CV as the resampling method.

```{r gbm1, echo=FALSE,warning=FALSE,message=FALSE}
set.seed(3000)
gbm1 = train(Reverse ~ .,data=Train,method="gbm",trControl =fitControl,verbose = FALSE)
gbm1
postProcess(gbm1,Test)
```

Changing the resampling method to 10-fold CV improves the accuracy to close to 0.72. Further tuning might improve the accuracy. The tuning parameters for gbm models are 
- interaction.depth
- n.trees
- shrinkage
- n.minobsinnode

```{r gbm2, echo=FALSE,warning=FALSE,message=FALSE}
gbmGrid <-  expand.grid(interaction.depth = c(1, 3, 5),
                       n.trees = (1:10)*10,
                       shrinkage = c(0.05,0.1,0.15),
                       n.minobsinnode = 10)
set.seed(3000)
gbm2 = train(Reverse ~ .,data=Train,method="gbm",trControl =fitControl, tuneGrid = gbmGrid,verbose = FALSE)
gbm2
postProcess(gbm2,Test)
```

The accuracy has come down slightly to 0.71. Of the boosting models in hand, the gbm1 model performs the best. But its still worth tuning to seek further improvement.

```{r gbm3, echo=FALSE,warning=FALSE,message=FALSE}
set.seed(3000)
gbm3 = train(Reverse ~ ., data = Train, method = "gbm",verbose = FALSE, 
             trControl = fitControl2, tuneGrid = gbmGrid, metric = "ROC")
gbm3
postProcess(gbm3,Test)
```

This boosting model gives almost as much accuracy as the best model we have had yet. To get a sense of the performance of few other popolar prediction algorithms, like Support Vector Machines, Regularized Discriminant Analysis, K nearest neighbours, Neural Network and Naive Bayes. We wont spend much time into tuning the models, instead the objective is to familarize its use for predictions.

#### Support Vector Machines

Lets begin with SVM.

```{r svm, echo=FALSE,message=FALSE,warning=FALSE}
set.seed(3000)
svm <- train(Reverse ~ ., data = Train, method = "svmRadial",allowParallel=F)
svm
postProcess(svm,Test)
```

The untuned SVM model gives a modest improvement in prediction accuracy over the baseline. Lets begin with 10-fold CV and scaling, centering to improve the accuracy.

```{r svm_cv, echo=FALSE,message=FALSE,warning=FALSE}
set.seed(3000)
svm1 <- train(Reverse ~ ., data = Train, method = "svmRadial",
                trControl = fitControl2,preProc = c("center", "scale"),allowParallel=F)
svm1
postProcess(svm1,Test)
```

This gives an increase in the accuracy, but its still lagging behind the accuracies of well tuned tree or boosting models. Lets use the tuneLength parameter to get an estimate of the 'C' parameter value.

```{r svm_tune, echo=FALSE,message=FALSE,warning=FALSE}
set.seed(3000)
svm2 <- train(Reverse ~ ., data = Train, method = "svmRadial",
                trControl = fitControl2,tuneLength = 16,metric = "ROC",
                preProc = c("center", "scale"),allowParallel=F)
svm2
postProcess(svm2,Test)
```

This again, has improved the accuracy by about 1%, but its still well behind the best models. We now check by manual tuning of the parameters to explore further improvements.

```{r svm_last, echo=FALSE,message=FALSE,warning=FALSE}
svmGrid <-  expand.grid(sigma = c(0.001, 0.01, 0.1), C = (1:10)*0.05)
set.seed(3000)
svm3 <- train(Reverse ~ ., data = Train, method = "svmRadial",
                trControl = fitControl2,tuneLength = 4,metric = "ROC",
                preProc = c("center", "scale"),tuneGrid = svmGrid,allowParallel=F)
svm3
postProcess(svm3,Test)
```

This gives a respectable prediction accuracy of 0.68. We now move on to other methods.

# Regularized Discriminant Analysis

Lets look at RDA.

```{r rda, echo=FALSE,message=FALSE,warning=FALSE}
set.seed(3000)
rda <- train(Reverse ~ ., data = Train, method = "rda",
                trControl = fitControl2, metric = "ROC",allowParallel=F)
rda
postProcess(rda,Test)
```

The rda model is providing accuracy of close to 0.73. Improvements to this might be possible with tuning of the gamma and lambda parameters. Displaying the tuning grid takes up too much space. SO we move to the confision matrix.

```{r rda1, echo=FALSE,warning=FALSE,message=FALSE}
rdaGrid <-expand.grid(gamma =  (0:8)/4, lambda =  (0:8)/4)
set.seed(3000)
rda1 <- train(Reverse ~ ., data = Train, method = "rda", metric = "ROC",
                trControl = fitControl2, tuneGrid = rdaGrid, allowParallel=F)
#rda1
postProcess(rda1,Test)
```

This leads to a drop in accuracy. It can be noted that the best gamma and lambda values gives improved training set predictions, but gives poor(er) test set predictions. This suggests possible overfitting.

#### K Nearest Neighbours

Lets proceed to KNN algorithm. Lets start with a 10 fold CV model with 'range' as an option for pre-processing

```{r knn, echo=FALSE,warning=FALSE,message=FALSE}
set.seed(3000)
knn <- train(Reverse ~ ., data=Train, method="knn", preProc=c("range"),
        trControl=fitControl, metric="Accuracy")
plot(knn)
knn
postProcess(knn,Test)
```

This gives a reasonable accuracy of 0.66. It could be possible to achieve more out of this model by tuning the value of k

```{r knn1, echo=FALSE,warning=FALSE,message=FALSE}
set.seed(3000)
knn1 <- train(Reverse ~ ., data=Train, method="knn", preProc=c("range"),
        trControl=fitControl, metric="Accuracy",tuneLength = 30)
plot(knn1)
knn1
postProcess(knn1,Test)
```

Approaching the 'correct' value of k, we improve the prediction to close to 0.68. 

#### Conditional Inference Trees

We move on to the next method, the Conditional Inference Trees, called ctree. It can be thought of as the tree models with significance test procedure in order to select variables instead of selecting the variable that maximizes an information measure (e.g. Gini coefficient).

```{r ctree, echo=FALSE,warning=FALSE,message=FALSE}
set.seed(3000)
ctree <- train(Reverse ~ ., data = Train, method = "ctree", 
                  trControl = fitControl2, preProc = c("center", "scale"))
ctree
plot(ctree$finalModel)
postProcess(ctree, Test)
```
We see that this tree is strikingly similar to the one from the tree model, as one might expect!

This gives close to 0.73 accuracy. It may be worth investigating further tuning of the model using the mincriterion parameter. We wont cover it here. 

#### C 4.5

We move on to what was the #1 in the [Top 10 Algorithms in Data Mining pre-eminent paper published by Springer LNCS in 2008]( #1 in the Top 10 Algorithms in Data Mining pre-eminent paper published by Springer LNCS in 200), C4.5

```{r c45, echo=FALSE,warning=FALSE,message=FALSE}
set.seed(3000)
c45 <- train(Reverse ~ ., data = Train, method = "J48", 
                  trControl = fitControl2,tuneGrid = expand.grid(C = seq(0.1,0.4,0.02)))
c45
postProcess(c45,Test)

```

This doesnt give as much accuracy as expected. The model may be further tuned by the 'C' parameter. 

#### Neural Network

Lets move on to Neural Networks. We begin with a no-tune model and work our way from there depending on the accuracy of this untuned model.

```{r nnet, echo=FALSE,warning=FALSE,message=FALSE}
set.seed(3000)
nn <- train(Reverse~.,data =Train, method = "nnet",trControl = fitControl, trace = F)
nn
postProcess(nn,Test)
```

Its worth using resampling to reduce overfitting and improve accuracy on the test set.

Using 10-fold CV as resampling...

```{r nnet1, echo=FALSE,warning=FALSE,message=FALSE}
set.seed(3000)
nn1 <- train(Reverse~.,data =Train, method = "nnet",
             trControl = fitControl2, trace = F,allowParallel=F)
nn1
postProcess(nn1,Test)
```

This gives improvement, but is worth checking if the model can be pushed further.

The output of the next model is too large to be helpful in this context. So we look at the confusion matrix directly.

```{r nnet2, echo=FALSE,warning=FALSE,message=FALSE}
set.seed(3000)
nn2 <- train(Reverse~.,data =Train, method = "nnet",
             trControl = fitControl2,tuneLength = 20, trace = F,allowParallel=F)
#nn2
postProcess(nn2,Test)
```

This gives a prediction accuracy of close to 0.68. 

#### Naive Bayes

Naive Bayes classifiers are a family of simple probabilistic classifiers based on applying Bayes' theorem with strong (naive) independence assumptions between the features.

```{r ,echo=FALSE,warning=FALSE,message=FALSE}
set.seed(3000)
nb = train(Reverse ~ ., data= Train, method ="nb",trControl = fitControl2)
nb
postProcess(nb,Test)
```


Naive Bayes performs poorly. We wont try tuning the NB model.

At this stage, we have explored a number of models. The best prediction performance seems to be in the order of 0.7, with the best being 0.74 from the CART model. Its surprising that a CART model outperforms Random Forest and Boosting models. 

With a quick wrap-up of what we have seen in this part, lets end it here.

### End of Part 1

This brings us to the end of part 1. In this section, we have 

- established the motivation for the work

- a brief description of the US courts setup

- 'cleaned' summary of the cases heard by Justice Stevens during the years of 1994-2001 

- strategy for selecting the training and test set

- establishing baseline prediction values that would serve as the guide to good predictions

- simple to complex tree based models ranging from trees to C4.5, their tuning and predictions on test set

- Support Vector Machines, KNN, Regularized Discriminant Analysis, Neural Network and their tuning and predictions.

- setting up parallel processing

The next parts would focus on diving a bit deeper into the predictions of some of the models, select the best model in terms of overall accuracy, and accuracies if individual Issues, Circuits, Petitioner and Respondent, i.e. explore if we can conclude if a particular model does well for a particular situation. We would also focus more on interpreting the results of a prediction



#### Footnote 
If the document is too large to be knit as html via the 'Knit HTML' option in R Studio, as it was the case for me, we can either

- add this snippet to the introduction section of the rmd file.
Output:
  html_document:
    pandoc_args: [
      "+RTS", "-K64m",
      "-RTS"
    ]

- or use the following code snippet on the r console:
library(knitr)
knit2html('courts.Rmd')



### Part 2: Interpretting the results and understanding the decisions 

In part 1 of this series, we discussed:

- motivation for the work

- brief description of the US courts setup.

- 'cleaned' summary of the cases heard by Justice Stevens during the years of 1994-2001 

- strategy for selecting the training and test set

- establishing baseline prediction values that would serve as the guide to good predictions

- simple to complex tree based models ranging from trees to C4.5, their tuning and predictions on test set

- Support Vector Machines, KNN, Regularized Discriminant Analysis, Neural Network and their tuning and predictions.

- setting up parallel processing

In this section, we start looking at the results of the predictions, interpretting the results, visualizing the decisions, implications and trade-offs of probability threshold.

#### CART Prediction Results

We started off with CART for its ease of interpretation. The interpretation is made easy by appropriate visualization. As such, the CART trained model, 'tree' is not an 'rpart' object. To view the tree better, we use the 'rpart' library to create a tree object of rpart class and plot it ising the prp function of the rpart.plot package.

```{r tree_viz,echo=FALSE, message=FALSE,warning=FALSE}
library(rpart)
library(rpart.plot)

rtree = rpart(Reverse~., data = Train, cp = 0.04444444)
prp(rtree)
```

A more colourful way to represent the tree is offered by the 'rattle' package, as shown below.

```{r tree_color, echo=FALSE,message=FALSE,warning=FALSE}
library(rattle)
fancyRpartPlot(rtree)
```

We see two ways of visualizing the decisions. Both convey the same meaning, one being more colourful than the other.

While we attempt to do this visualization, its important to keep in mind that this visualization does not describe the ideologies and decision making process of the judge in this causal way. Instead it looks for patterns and explains the outcomes of the training set based on these patterns.

Lets look at interpreting the visualization. 

The first split or deicsion is on the ideological direction of the lower court, represented by variable 'LowerCourt' and takes either 'liberal' or 'conservative'. If the lower court's ideological direction is not liberal (conservative, which happens in 52% of the cases), then the model predicts that Justics Stevens will reverse the decision of the lower court. 

The remaining 48% of the cases whose lower court's ideological direction is liberal, reaches another decision point. If the Circuit of origin of the case. If the circuit is 2nd, 5th, 6th, 9th or the FED, the lower court ruling is reversed. This happens in approximately 27% of the training set cases. The cases from Circuits 1,3,4,7,8,10,11 and DC, which comprise 22% of the Train data, the lower court ruling is affirmed by Justics Stevens. 


The fractions in each of the box denotes the probability of a 'yes' or 'no' (reverse or affirm lower court decision). The probability of yes is given in the right and the probability of a no is given in the left side of the box.

#### Goodness of the model

An important parameter in evaluating the goodness of the model is the ROC curve. Receiver operating characteristic, or ROC curve, is a graphical plot that illustrates the performance of a binary classifier system as its discrimination threshold is varied. The curve is created by plotting the true positive rate against the false positive rate at various threshold settings. The true-positive rate is also known as sensitivity. False-positive rate can be calculated as (1 - specificity). 

The area under a ROC curve quantifies the overall ability of the test to discriminate between those cases whose lower court decision is to be reversed and those that are to be affirmed. A truly random prediction (one no better at identifying true positives than flipping a coin) has an area of 0.5. A perfect prediction (one that has zero false positives and zero false negatives) has an area of 1.00. Our AUC will be inbetween these numbers, but we would aspire to have the ROC curve 'hug' the north-west corner of the ROC plot to get higher AUC.

We use the ROCR package to get the ROC curve for this tree model.

```{r tree_roc, warning=FALSE, message=FALSE, echo=FALSE}
library(ROCR)

PredictROC = predict(tree, newdata = Test, type = "prob")
pred = prediction(PredictROC[,2], Test$Reverse)
perf = performance(pred, "tpr", "fpr")
plot(perf,main=('ROC for Tree mode- tree'))
grid(col = "lightgray", lty = "dotted")
auc_tree = as.numeric(performance(pred, "auc")@y.values)

print(paste0("The Area under the ROC Curve- AUC of the untuned Tree mode 'tree' = ", as.character(round(auc_tree,4))))

```

This nicely illustrates the concepts of sensitivity and specificity. A generalization of the model performance unser various combinations of Sensitivity and Specificity is the ROC. Lets try to look at a specific example to understand the Sensitivity and Specificity, in context of prediction accuracy.

#### Manual tweeks to Probability Thresholds: The Sensitivity-Specificity Trade off

Lets go back to the random forest models, rf, rf1 and rf2 from Part 1 of this series. One may recall that these models give lower accuracy than the CART models. In this section, we explore if the Sensitivity and Specificity can be manipulated to achieve better prediction accuracy. One tool to manipulate them with is the class probability threshold.

The random forest model select the prediction class based on the probability of the prediction for decision reversal. The default threshold for selecting a class is 0.5. So if the probability for a class is >= 0.5, we choose the corresponding class. So if the probability for reversal is >= 0.5, the model selects 'yes' for reversal.

We can try changing this default of 0.5 and see if this affects the prediction quality. It has to be kept in mind that this can affect, in some cases, severely, the sensitivity and specificity of the outcomes. Such manipulation of the probabilities can be helpful if one of the outcomes is more important (or carries financial penalties) than the other. But in out case, both the outcomes mean the same and we do not have a preference. But lets examine the performance if we relax the class selection probability by +/- 0.05.

Lets begin with rf. First we see the confusion matrix for rf1 with default settings, then the probability threshold set to 0.55, and then to 0.45

```{r rf_prob, echo=FALSE,warning=FALSE,message=FALSE}

# rf model
pred.rf <- predict(rf, newdata = Test,type="prob")

confusionMatrix(Test$Reverse,factor(ifelse(pred.rf[,2]>0.5,"yes","no")))

confusionMatrix(Test$Reverse,factor(ifelse(pred.rf[,2]>0.55,"yes","no")))

confusionMatrix(Test$Reverse,factor(ifelse(pred.rf[,2]>0.45,"yes","no")))
```

Now, the same for rf1...

```{r rf1_prob, echo=FALSE,warning=FALSE,message=FALSE}
pred.rf1 <- predict(rf1, newdata = Test,type="prob")

confusionMatrix(Test$Reverse,factor(ifelse(pred.rf1[,2]>0.5,"yes","no")))

confusionMatrix(Test$Reverse,factor(ifelse(pred.rf1[,2]>0.55,"yes","no")))

confusionMatrix(Test$Reverse,factor(ifelse(pred.rf1[,2]>0.45,"yes","no")))
```

and for rf2...

```{r rf2_prob, echo=FALSE,warning=FALSE,message=FALSE}
pred.rf2 <- predict(rf2, newdata = Test,type="prob")

confusionMatrix(Test$Reverse,factor(ifelse(pred.rf2[,2]>0.5,"yes","no")))

confusionMatrix(Test$Reverse,factor(ifelse(pred.rf2[,2]>0.55,"yes","no")))

confusionMatrix(Test$Reverse,factor(ifelse(pred.rf2[,2]>0.45,"yes","no")))
```

We see the prediction accuracy increases to around 0.7 as we change the class probability threshold from 0.5 to 0.5 +/- 0.05. But as one might expect with this change, the specificity increases at the cost of sensitivity. 

Lets take a step aside and define sensitivity and specificity in context of this problem.

Specificity- the proportion of predicted reversals to actual reversals.

Sensitivity-  the proportion of predicted non-reversals to actual non-reversals.

It has to be kept in mind that this increase in threshold from 0.5 to 0.55 means the model can miss to identify the reversals, but improves on the correct prediction of non-reversals. A graphical of representing this is the ROC curve. So we see that resampling, tuning and manipulating the sensitivity-specificity, random forests gives predictions with an accuracy of close to 0.72

This brings us to the end of part 2 of this 3 part series. In this section we have covered:

- discussion of results of the predictions from part 1
- interpretting the results
- visualizing the decisions
- implications and trade-offs of class selection probability threshold

In the third and final part, we will look into model selection and concluse the study.


### Part 3: Model Selection and Conclusion

In part 2 of the series, we discussed the results of the predictions from part 1, interpretting the results, vizualizing the decisions and the sensitivity-specificity trade-off.

In this section we will look into model selection and conclude our study.

First, lets look at the summary of all the models with us right now.



| Model | Resampling |  Metric  | Training Set Accuracy | Test Set Accuracy |  Summary Function | PreProcessing | Manual Tune |
|:-----:|:----------:|:--------:|:---------------------:|:-----------------:|:-----------------:|:-------------:|:-----------:|
|  tree |   default  | accuracy |          0.62         |        0.74       |         no        |       no      |      no     |
| tree1 |   default  | accuracy |          0.64         |        0.74       |         no        |       no      |      no     |
| tree2 | 10-fold CV | accuracy |          0.64         |        0.74       |         no        |       no      |      no     |
|   rf  |   default  | accuracy |          0.62         |        0.66       |         no        |       no      |      no     |
|  rf1  | 10-fold CV | accuracy |          0.65         |        0.68       |         no        |       no      |      no     |
|  rf2  | 10-fold CV | accuracy |          0.62         |        0.69       |         no        |  scale+center |      no     |
|   cf  |   default  | accuracy |          0.62         |        0.73       |         no        |       no      |      no     |
|  cf1  | 10-fold CV |    ROC   |          0.67         |        0.73       | Two Class Summary |       no      |      no     |
|  gbm  |   default  | accuracy |          0.61         |        0.69       |         no        |       no      |      no     |
|  gbm1 | 10-fold CV | accuracy |          0.64         |        0.73       |         no        |       no      |      no     |
|  gbm2 | 10-fold CV | accuracy |          0.64         |        0.71       |         no        |       no      |     yes     |
|  gbm3 | 10-fold CV |    ROC   |          0.68         |        0.73       | Two Class Summary |       no      |      no     |
|  svm  |   default  | accuracy |          0.61         |        0.6        |         no        |       no      |      no     |
|  svm1 | 10-fold CV |    ROC   |          0.68         |        0.64       | Two Class Summary |  scale+center |      no     |
|  svm2 | 10-fold CV |    ROC   |          0.68         |        0.65       | Two Class Summary |  scale+center |     yes     |
|  svm3 | 10-fold CV |    ROC   |          0.69         |        0.69       | Two Class Summary |  scale+center |     yes     |
|  rda  | 10-fold CV |    ROC   |          0.67         |        0.74       | Two Class Summary |       no      |      no     |
|  rda1 | 10-fold CV |    ROC   |          0.69         |        0.68       | Two Class Summary |       no      |     yes     |
|  knn  | 10-fold CV | accuracy |          0.63         |        0.66       |         no        |     range     |      no     |
|  knn1 | 10-fold CV | accuracy |          0.64         |        0.69       |         no        |     range     |     yes     |
| ctree | 10-fold CV |    ROC   |          0.59         |        0.65       | Two Class Summary |       no      |     yes     |
|  c45  | 10-fold CV |    ROC   |          0.59         |        0.65       | Two Class Summary |       no      |     yes     |
|   nn  | 10-fold CV | accuracy |          0.64         |        0.67       |         no        |       no      |      no     |
|  nn1  | 10-fold CV |    ROC   |          0.68         |        0.69       | Two Class Summary |       no      |      no     |
|  nn2  | 10-fold CV |    ROC   |          0.63         |        0.69       | Two Class Summary |       no      |     yes     |
|   nb  | 10-fold CV |    ROC   |          0.68         |        0.54       | Two Class Summary |       no      |      no     |



Lets look into it in a bit more detail. Lets get some graphical representation of the same data. 

```{r summ, echo=FALSE,message=FALSE,warning=FALSE}
a = read.table("model_summary.txt", header = T, sep = "|")
a$X = NULL
a$X.1 = NULL
str(a)
trim <- function (x) gsub("^\\s+|\\s+$", "", x)
a$Model <- trim(as.character(a$Model))
a$Model <- as.factor(a$Model)
library(ggplot2)
```

Lets plot the training set accuracies first.

```{r trg_plot,echo=FALSE,warning=FALSE,message=FALSE}
ggplot(data=a, aes(x=Model, y=Training.Set.Accuracy, group=1, fill=Metric)) + geom_bar(stat="identity") + expand_limits(y=0) +
    xlab("Models") + ylab("Training Set accuracy") +
    ggtitle("Training Set Accuracy/ROC of various models")
```

Lets now plot the test set accuracies.

```{r test_plot,echo=FALSE,warning=FALSE,message=FALSE}
ggplot(data=a, aes(x=Model, y=Test.Set.Accuracy, group=1)) + geom_bar(stat="identity") + expand_limits(y=0) +
    xlab("Models") + ylab("Test Set Prediction Accuracy") +
    ggtitle("Test Set Prediction Accuracy of various models")
```

Lets now look at models which gave better test set accuracy than the training set

```{r ratio,echo=FALSE,message=FALSE,warning=FALSE}

Possible.Overfitting = ifelse(a$Test.Set.Accuracy/a$Training.Set.Accuracy > 1, "False","True")

ggplot(data=a, aes(x=Model, y=Test.Set.Accuracy/Training.Set.Accuracy, group=1, fill=Possible.Overfitting)) + geom_bar(stat="identity") + expand_limits(y=0) +
    xlab("Models") + ylab("Training Set accuracy") +
    ggtitle("Training Set Accuracy/ROC of various models")

```


This gives an idea of which models do well and which ones dont do as well. The easy way out is to take the model(s) that performs well or is expected to perform well based on prior experience with the model.

#### Final Models

Lets consider the following models for our final model selection:
- tree
- cf
- gbm1
- svm3
- nn2
- knn1

Lets see how each of these selected models do in the overall prediction accuracy, but also the categorical predictions, i.e. how good is each model in predicting cases that are from different circuits, different Respondents ect. This is important for this study as we expect cases from all types of respondents, from different places and if we can identify models that work for each sub-category well, we can improve our overall accuracy. This method is used in place of the more structured approach given [here](http://topepo.github.io/caret/training.html)

Lets explore by looking into the predictions of the each model and their breakdown for individual sub-categories.

```{r categorical.results,echo=FALSE,warning=FALSE,message=FALSE}
library(dplyr)
# tree
predicttree = predict(tree, newdata = Test)
tree.df = mutate(Test, prediction = ifelse(predicttree == Test$Reverse,1,0))
str(tree.df)
tree.result= sapply(tree.df[,1:6],function(x){tapply(tree.df$prediction,x,mean)})

# cf
predict.cf = predict(cf, newdata = Test)
cf.df = mutate(Test, prediction = ifelse(predict.cf == Test$Reverse,1,0))
str(cf.df)
cf.result = sapply(cf.df[,1:6],function(x){tapply(cf.df$prediction,x,mean)})

# gbm1
predict.gbm1 = predict(gbm1, newdata = Test)
gbm1.df = mutate(Test, prediction = ifelse(predict.gbm1 == Test$Reverse,1,0))
str(gbm1.df)
gbm1.result= sapply(gbm1.df[,1:6],function(x){tapply(gbm1.df$prediction,x,mean)})

# svm3
predict.svm3 = predict(svm3, newdata = Test)
svm3.df = mutate(Test, prediction = ifelse(predict.svm3 == Test$Reverse,1,0))
str(svm3.df)
svm3.result= sapply(svm3.df[,1:6],function(x){tapply(svm3.df$prediction,x,mean)})

# nn2
predict.nn2 = predict(nn2, newdata = Test)
nn2.df = mutate(Test, prediction = ifelse(predict.nn2 == Test$Reverse,1,0))
str(nn2.df)
nn2.result= sapply(nn2.df[,1:6],function(x){tapply(nn2.df$prediction,x,mean)})

# knn1
predict.knn1 = predict(knn1, newdata = Test)
knn1.df = mutate(Test, prediction = ifelse(predict.knn1 == Test$Reverse,1,0))
str(knn1.df)
knn1.result= sapply(knn1.df[,1:6],function(x){tapply(knn1.df$prediction,x,mean)})

res = cbind(tree.result,cf.result,gbm1.result,svm3.result,nn2.result,knn1.result)

colnames(res) <- c("tree","cf","gbm1","svm3","nn2","knn1")
res = data.frame(res)

```


Lets tabulate and visualize the categorical results. It has to be kept in mind that the following recommendations are valid for the respective Circuits, Issues, etc. IN ISOLATION. No interaction effect is considered. Lets begin by looking at the Circuits.

##### Circuits

| Circuit | 1st   | 2nd   | 3rd   | 4th   | 5th   | 6th   | 7th   | 8th   | 9th   | 10th  | 11th  | DC    | FED   | Overall Accuracy |
|---------|-------|-------|-------|-------|-------|-------|-------|-------|-------|-------|-------|-------|-------|------------------|
| Tree    | 0.571 | 0.556 | 0.714 | 1.000 | 0.769 | 0.786 | 0.556 | 0.813 | 0.676 | 0.833 | 0.818 | 0.714 | 0.571 | 0.740            |
| CF      | 0.571 | 0.556 | 0.714 | 1.000 | 0.769 | 0.786 | 0.556 | 0.813 | 0.618 | 0.833 | 0.818 | 0.714 | 0.571 | 0.728            |
| GBM1    | 0.571 | 0.667 | 0.714 | 1.000 | 0.769 | 0.786 | 0.556 | 0.813 | 0.647 | 0.750 | 0.818 | 0.571 | 0.571 | 0.728            |
| SVM3    | 0.429 | 0.778 | 0.643 | 0.938 | 0.769 | 0.714 | 0.444 | 0.688 | 0.618 | 0.750 | 0.818 | 0.571 | 0.571 | 0.686            |
| NN2     | 0.571 | 0.667 | 0.643 | 1.000 | 0.692 | 0.643 | 0.556 | 0.813 | 0.588 | 0.750 | 0.818 | 0.429 | 0.571 | 0.686            |
| KNN1    | 0.429 | 0.556 | 0.429 | 1.000 | 0.769 | 0.786 | 0.556 | 0.813 | 0.588 | 0.583 | 0.818 | 0.857 | 0.571 | 0.686            |

```{r circuits,echo=FALSE,message=FALSE,warning=FALSE}
circuits <- read.csv("circuits.csv")
library(reshape2)
names(circuits)[1] <- "model"
circ <- melt(circuits, id.vars = c("model"), variable.name = "circuit", value.name = "accuracy")
library(ggplot2)
qplot(data=circ,x=model,y=accuracy,color=circuit,facets = ~circuit,xlab="Models", ylab="Accuracy", main= "Accuracy of each model for different Circuits", ylim = c(0, 1) )
```


The plot shows that most of the selected models give accuracies in similar order of magnitude. Noteable deviations from this trend are observed in Circuits 3, 10 and DC. When decisions from these circuits are important, its recomended to avoid using the models giving the low accuracies for the respective circuits.

##### Issues:

| Issue | Attorneys | CivilRights | CriminalProcedure | DueProcess | EconomicActivity | FederalismAndInterstateRelations | FederalTaxation | FirstAmendment | JudicialPower | Privacy | Unions | Overall Accuracy |
|-------|-----------|-------------|-------------------|------------|------------------|----------------------------------|-----------------|----------------|---------------|---------|--------|------------------|
| Tree  | NA        | 0.778       | 0.750             | 0.909      | 0.643            | 0.750                            | 0.667           | 0.900          | 0.727         | 0.333   | 0.750  | 0.740            |
| CF    | NA        | 0.778       | 0.682             | 0.909      | 0.607            | 0.750                            | 0.833           | 0.900          | 0.727         | 0.667   | 0.750  | 0.728            |
| GBM1  | NA        | 0.778       | 0.727             | 0.909      | 0.679            | 0.750                            | 0.833           | 0.700          | 0.667         | 0.667   | 0.750  | 0.728            |
| SVM3  | NA        | 0.778       | 0.659             | 1.000      | 0.607            | 0.625                            | 0.333           | 0.800          | 0.697         | 0.667   | 0.625  | 0.686            |
| NN2   | NA        | 0.778       | 0.659             | 0.909      | 0.643            | 0.750                            | 0.333           | 0.600          | 0.697         | 0.667   | 0.750  | 0.686            |
| KNN1  | NA        | 0.667       | 0.659             | 0.818      | 0.607            | 1.000                            | 0.833           | 0.500          | 0.667         | 1.000   | 0.625  | 0.686            |


```{r issue,echo=FALSE,message=FALSE,warning=FALSE}
issue <- read.csv("issues.csv")
issues <- melt(issue, id.vars = c("model"), variable.name = "issue", value.name = "accuracy")
qplot(data=issues,x=model,y=accuracy,color=issue,facets = ~issue,xlab="Models", ylab="Accuracy", main= "Accuracy of each model for different Issues", ylim = c(0, 1))
```

A similar plot for Issues reveal a similar trend, with a clear difference in the the Attourneys section. There are no values to plot. Meaning that there are no Attourney Issues in the Test set. In the other Issues, we see that DueProcess, FederalismAndInterstateRelations, FederalTaxation, FirstAmendment and Privacy show varying levsls of accuracy differences. Depending on which Issue is important, we choose a suitable model to give best prediction accuracy.

 
##### Respondent

| Respondent | AMERICAN.INDIAN | BUSINESS | CITY  | CRIMINAL.DEFENDENT | EMPLOYEE | EMPLOYER | GOVERNMENT.OFFICIAL | INJURED.PERSON | OTHER | POLITICIAN | STATE | US    | Overall Accuracy |
|------------|-----------------|----------|-------|--------------------|----------|----------|---------------------|----------------|-------|------------|-------|-------|------------------|
| Tree       | 0.333           | 0.619    | 1.000 | 0.813              | 0.833    | 1.000    | 0.750               | 1.000          | 0.650 | 1.000      | 0.947 | 0.684 | 0.740            |
| CF         | 0.333           | 0.714    | 1.000 | 0.813              | 0.667    | 1.000    | 0.625               | 1.000          | 0.633 | 1.000      | 0.947 | 0.632 | 0.728            |
| GBM1       | 0.333           | 0.762    | 1.000 | 0.813              | 0.667    | 1.000    | 0.750               | 1.000          | 0.600 | 0.800      | 0.947 | 0.684 | 0.728            |
| SVM3       | 0.333           | 0.667    | 0.500 | 0.875              | 0.500    | 0.714    | 0.750               | 0.333          | 0.617 | 1.000      | 0.947 | 0.579 | 0.686            |
| NN2        | 0.667           | 0.714    | 1.000 | 0.688              | 0.667    | 0.857    | 0.500               | 1.000          | 0.600 | 0.800      | 0.947 | 0.579 | 0.686            |
| KNN1       | 0.667           | 0.667    | 1.000 | 0.813              | 0.667    | 0.857    | 0.750               | 1.000          | 0.533 | 0.800      | 0.895 | 0.632 | 0.686            |


```{r respo,echo=FALSE,message=FALSE,warning=FALSE}
resp <- read.csv("respon.csv")
respon <- melt(resp, id.vars = c("model"), variable.name = "respon", value.name = "accuracy")
qplot(data=respon,x=model,y=accuracy,color=respon,facets = ~respon,xlab="Models", ylab="Accuracy", main= "Accuracy of each model for different Respondent", ylim = c(0, 1))
```

A glance at the plot reveal that if the respondent is Americal.Indian, KNN1 and NN2 gives best predictions. If the Respondent is City, all models except SVM3 gives similarly good predictions. Predictions for Injured.Person is consistently good, barring the SVM3 model. We have seen that SVM3 model has featured in 2 of the low prediction cases in Respondents. So SVM3 model can be removed as a choice of predictor if Respondent is an important requirement.

##### Petitioner

| Petitioner | AMERICAN.INDIAN | BUSINESS | CITY  | CRIMINAL.DEFENDENT | EMPLOYEE | EMPLOYER | GOVERNMENT.OFFICIAL | INJURED.PERSON | OTHER | POLITICIAN | STATE | US    | Overall Accuracy |
|------------|-----------------|----------|-------|--------------------|----------|----------|---------------------|----------------|-------|------------|-------|-------|------------------|
| Tree       | 1.000           | 0.650    | 0.500 | 0.793              | 0.875    | 0.667    | 0.615               | 1.000          | 0.724 | 1.000      | 0.750 | 0.733 | 0.740            |
| CF         | 1.000           | 0.550    | 0.500 | 0.793              | 0.875    | 0.667    | 0.462               | 1.000          | 0.741 | 1.000      | 0.750 | 0.800 | 0.728            |
| GBM1       | 1.000           | 0.650    | 0.500 | 0.793              | 0.875    | 0.667    | 0.538               | 1.000          | 0.690 | 1.000      | 0.750 | 0.800 | 0.728            |
| SVM3       | 1.000           | 0.600    | 0.500 | 0.724              | 0.750    | 0.667    | 0.462               | 0.667          | 0.707 | 1.000      | 0.750 | 0.667 | 0.686            |
| NN2        | 1.000           | 0.650    | 0.500 | 0.724              | 0.750    | 0.667    | 0.385               | 1.000          | 0.707 | 1.000      | 0.750 | 0.600 | 0.686            |
| KNN1       | 1.000           | 0.550    | 0.000 | 0.793              | 0.750    | 0.667    | 0.385               | 1.000          | 0.690 | 0.800      | 0.750 | 0.733 | 0.686            |


```{r petit,echo=FALSE,warning=FALSE,message=FALSE}
petit <- read.csv("petit.csv")
pet <- melt(petit, id.vars = c("model"), variable.name = "petit", value.name = "accuracy")
qplot(data=pet,x=model,y=accuracy,color=petit,facets = ~petit,xlab="Models", ylab="Accuracy", main= "Accuracy of each model for different Petitioner", ylim = c(0, 1))
```

Its immediately obvious that all the models have low prediction accuracy if the Petitioner is Business, Government.Official and City. KNN1 performs especially bad for City. So KNN1 can be eliminated as a helpful model to predict cases for which the Petitioner is important. On similar grounds, SVM3 and NN2 can be eliminated too.

##### Lower COurt Direction

| Lower Court Direction | conser | liberal | Overall Accuracy |
|-----------------------|--------|---------|------------------|
| Tree                  | 0.773  | 0.704   | 0.740            |
| CF                    | 0.773  | 0.679   | 0.728            |
| GBM1                  | 0.750  | 0.704   | 0.728            |
| SVM3                  | 0.682  | 0.691   | 0.686            |
| NN2                   | 0.705  | 0.667   | 0.686            |
| KNN1                  | 0.761  | 0.593   | 0.686            |

Since this has only 2 sub-categories, it can be easily inferred from without a visual representation of it.

Tree model performs best overall, followed by CF and GBM1.


##### Unconstititional

| Unconst | no / 0 | yes / 1 | Overall Accuracy |
|---------|--------|---------|------------------|
| Tree    | 0.693  | 0.881   | 0.740            |
| CF      | 0.693  | 0.833   | 0.728            |
| GBM1    | 0.701  | 0.810   | 0.728            |
| SVM3    | 0.677  | 0.714   | 0.686            |
| NN2     | 0.654  | 0.786   | 0.686            |
| KNN1    | 0.646  | 0.786   | 0.686            |

Similarly, this table can be interpreted too.

#### Final Model

Overall, the CART model performs the best, both in terms of overall predictions and sub-categorical prediction. This is very closely followed by Random Forest and Boosting models. Usually, one would RF and GBM models to be more accurate that the CART model.

The KNN, SVM and NNET models give good accuracy too, but for some sub-categories, it gives lower accuracy that the Tree based models.


### Conclusion

This study was intended to statistically assess and predict the US Supreme Court decision making. We found that the models predict cose to 75% of the cases correctly.

The statistical models deems that the factors and only these factors are necessary to understand and model Supreme Court decision making. Although the model incorporated a large number of past results in its analysis, it took no account of the explanations the Court itself gave for those decisions. Nor did it take into account specific precedent or relevant statutory or constitutional text. The model is essentially nonlegal in that the factors used to predict decisions-the circuit of origin, the type of the petitioner and respondent, and so forth-are indifferent to law.

In this context, the motivation for this study was to determine whether there the observable "legal" factors are important and can be used for the prediction of outcomes. Our results show that it is possible to predict outcomes of cases if the "legal" factors are coded and available.

In the manner suggested above, we think that the results of this comparative study provide interesting additive insights into the manner in
which one might conceptualize the Supreme Court's decisionmaking. 

This concludes the 3 part series on the prediction of US Supreme Court decisions.